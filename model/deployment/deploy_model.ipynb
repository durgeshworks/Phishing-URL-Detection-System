{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example snippet to update lambda/lambda_function.py programmatically (prints instructions, does not edit files)\n",
    "print('Open lambda/lambda_function.py and set EndpointName to:', ENDPOINT_NAME)\n",
    "print('\\nEnsure the Lambda execution role has:')\n",
    "print('- sagemaker:InvokeEndpoint')\n",
    "print('- dynamodb:PutItem on table PhishingDetections')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad535c8d",
   "metadata": {},
   "source": [
    "### Update Lambda with endpoint name\n",
    "\n",
    "If you deploy to SageMaker, update `lambda/lambda_function.py` so `EndpointName` matches `ENDPOINT_NAME` above. Also ensure the Lambda role has permission to invoke the SageMaker endpoint and to write to DynamoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb21f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CSV body using the same feature extractor\n",
    "def features_to_csv(url: str):\n",
    "    X = extract_features(url).reshape(-1)\n",
    "    return ','.join(map(str, X.tolist()))\n",
    "\n",
    "sample_url = 'http://phishy.example.com/login'\n",
    "body = features_to_csv(sample_url)\n",
    "\n",
    "resp = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=ENDPOINT_NAME,\n",
    "    ContentType='text/csv',\n",
    "    Body=body\n",
    ")\n",
    "\n",
    "# Many runtime responses are bytes; decode and print\n",
    "result = resp['Body'].read().decode()\n",
    "print('Raw runtime response:', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ecd82f",
   "metadata": {},
   "source": [
    "### Invoke the endpoint using sagemaker-runtime\n",
    "\n",
    "This cell demonstrates invoking the endpoint with CSV body matching the feature vector format used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dccfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = sagemaker.create_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME,\n",
    "        EndpointConfigName=ENDPOINT_CONFIG_NAME\n",
    "    )\n",
    "    print('Create endpoint response:', resp)\n",
    "except sagemaker.exceptions.ClientError as e:\n",
    "    print('Create endpoint failed or already exists:', e)\n",
    "\n",
    "# Wait for endpoint to be InService\n",
    "print('Waiting for endpoint to be InService... (this may take several minutes)')\n",
    "waiter = sagemaker.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=ENDPOINT_NAME)\n",
    "print('Endpoint is InService')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2d3c7",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "\n",
    "Create or update the endpoint. This operation can take several minutes depending on the instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resp = sagemaker.create_endpoint_config(\n",
    "        EndpointConfigName=ENDPOINT_CONFIG_NAME,\n",
    "        ProductionVariants=[{\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': MODEL_NAME,\n",
    "            'InitialInstanceCount': INSTANCE_COUNT,\n",
    "            'InstanceType': INSTANCE_TYPE,\n",
    "            'InitialVariantWeight': 1\n",
    "        }]\n",
    "    )\n",
    "    print('Endpoint config created:', resp)\n",
    "except sagemaker.exceptions.ClientError as e:\n",
    "    print('Create endpoint config failed or already exists:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb351ae7",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "\n",
    "Create an endpoint config referencing the model name and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c859ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_payload = {\n",
    "    'ModelName': MODEL_NAME,\n",
    "    'PrimaryContainer': {\n",
    "        'Image': IMAGE_URI,\n",
    "        'ModelDataUrl': S3_MODEL_ARTIFACT\n",
    "    },\n",
    "    'ExecutionRoleArn': ROLE_ARN\n",
    "}\n",
    "\n",
    "try:\n",
    "    resp = sagemaker.create_model(**create_model_payload)\n",
    "    print('Create model response:', resp)\n",
    "except sagemaker.exceptions.ClientError as e:\n",
    "    print('Create model failed or model already exists:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b316f3",
   "metadata": {},
   "source": [
    "### Create model (Using a custom container)\n",
    "\n",
    "If you're using a custom inference container, register the model pointing to the ECR image and S3 model artifact. For scikit-learn models you can use the pre-built SKLearn containers or package a custom container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Configuration variables - update before running\n",
    "S3_MODEL_ARTIFACT = 's3://your-bucket/path/to/model.tar.gz'  # artifact tarball containing model.joblib or framework-specific model\n",
    "MODEL_NAME = 'phishing-detector-model-1'\n",
    "ENDPOINT_CONFIG_NAME = 'phishing-detector-endpoint-config-1'\n",
    "ENDPOINT_NAME = 'phishing-detector-endpoint-1'\n",
    "ROLE_ARN = 'arn:aws:iam::123456789012:role/SageMakerExecutionRole'  # replace with your role\n",
    "IMAGE_URI = '123456789012.dkr.ecr.us-west-2.amazonaws.com/phishing-detector:latest'  # for custom container\n",
    "INSTANCE_TYPE = 'ml.t2.medium'\n",
    "INSTANCE_COUNT = 1\n",
    "\n",
    "print('SageMaker client ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3123755",
   "metadata": {},
   "source": [
    "## SageMaker deployment (optional)\n",
    "\n",
    "The cells below show how to register the model with SageMaker, create an endpoint configuration, create an endpoint, wait for it to be InService, and invoke it using the `sagemaker-runtime` client. These operations require AWS credentials configured in the environment with permissions for SageMaker and S3.\n",
    "\n",
    "Important: this notebook does not create IAM roles. Use an existing SageMaker execution role ARN with appropriate permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, subprocess\n",
    "\n",
    "# Save model with metadata\n",
    "os.makedirs('../output', exist_ok=True)\n",
    "artifact_path = '../output/model.joblib'\n",
    "if 'model' in globals():\n",
    "    joblib.dump(model, artifact_path)\n",
    "    commit_hash = subprocess.check_output(['git','rev-parse','--short','HEAD']).decode().strip() if os.path.exists('.git') else 'local'\n",
    "    metadata = {'artifact': os.path.basename(artifact_path), 'version': '1.0', 'commit': commit_hash}\n",
    "    with open('../output/model.metadata.json','w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    print('Saved model and metadata to ../output')\n",
    "else:\n",
    "    print('No model in memory to save. Copy your trained artifact to ../output/model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e9e3d",
   "metadata": {},
   "source": [
    "## 13) Save and version model artifacts\n",
    "\n",
    "Save the model artifact alongside metadata (version, commit hash) and optionally upload to S3 or an artifact store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897e60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_yaml = '''\n",
    "name: CI\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test-and-build:\n",
    "    runs-on: ubuntu-latest\n",
    "    strategy:\n",
    "      matrix:\n",
    "        python-version: [3.8, 3.9]\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: ${{ matrix.python-version }}\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "      - name: Run tests\n",
    "        run: |\n",
    "          pytest -q\n",
    "      - name: Build Docker image\n",
    "        run: |\n",
    "          docker build -t phishing-detector:${{ github.sha }} .\n",
    "'''\n",
    "\n",
    "print('Sample workflow YAML written to variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a618f9b1",
   "metadata": {},
   "source": [
    "## 12) Automated tests: CI workflow (GitHub Actions)\n",
    "\n",
    "A sample workflow to run tests and build the Docker image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed13b88",
   "metadata": {},
   "source": [
    "## 11) Build and run Docker container (commands)\n",
    "\n",
    "Example commands (run in PowerShell):\n",
    "\n",
    "```powershell\n",
    "docker build -t phishing-detector:latest .\n",
    "docker run -p 8000:8000 --rm phishing-detector:latest\n",
    "```\n",
    "\n",
    "Then call the endpoint:\n",
    "\n",
    "```powershell\n",
    "curl -X POST http://127.0.0.1:8000/predict -H \"Content-Type: application/json\" -d '{\"url\":\"https://example.com/login\"}'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9714ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile_content = '''\n",
    "FROM python:3.9-slim\n",
    "WORKDIR /app\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY model_server.py ./\n",
    "COPY model.joblib ./\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\",\"model_server:app\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"]\n",
    "'''\n",
    "with open('Dockerfile','w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "print('Wrote Dockerfile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d1f453",
   "metadata": {},
   "source": [
    "## 10) Create Dockerfile for containerized deployment\n",
    "\n",
    "Create a Dockerfile that installs dependencies, copies the model and app, and runs uvicorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331da7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run uvicorn in a subprocess (suitable for local dev; stop manually when done)\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "uvicorn_proc = subprocess.Popen(['uvicorn','model_server:app','--host','127.0.0.1','--port','8000'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "print('Started uvicorn PID', uvicorn_proc.pid)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# call predict\n",
    "import requests\n",
    "resp = requests.post('http://127.0.0.1:8000/predict', json={'url': example_url})\n",
    "print('Status:', resp.status_code)\n",
    "print('Response:', resp.json())\n",
    "\n",
    "# Terminate uvicorn\n",
    "uvicorn_proc.terminate()\n",
    "uvicorn_proc.wait()\n",
    "print('Stopped uvicorn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9070318",
   "metadata": {},
   "source": [
    "## 9) Run API locally with Uvicorn\n",
    "\n",
    "Run the FastAPI app locally and call the /predict endpoint using `requests`. The cell below runs uvicorn in the background (note: notebook kernels differ in handling background processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat > model_server.py <<'PY'\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "class InputSchema(BaseModel):\n",
    "    url: str\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    url: str\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "MODEL_PATH = 'model.joblib'\n",
    "\n",
    "try:\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "except Exception:\n",
    "    model = None\n",
    "\n",
    "@app.get('/health')\n",
    "async def health():\n",
    "    return {'status':'ok', 'model_loaded': model is not None}\n",
    "\n",
    "@app.post('/predict', response_model=OutputSchema)\n",
    "async def predict(payload: InputSchema):\n",
    "    url = payload.url\n",
    "    # feature extraction - same as training\n",
    "    X = np.array([len(url), url.count('.'), url.count('-'), url.count('@'), int('https' in url), int('login' in url)]).reshape(1,-1)\n",
    "    if model is None:\n",
    "        return {'url': url, 'prediction': 'error', 'confidence': 0.0}\n",
    "    if hasattr(model,'predict_proba'):\n",
    "        proba = model.predict_proba(X)\n",
    "        score = float(proba[0,1]) if proba.shape[1]>1 else float(proba[0,0])\n",
    "        label = 'phishing' if score>0.5 else 'legit'\n",
    "    else:\n",
    "        pred = model.predict(X)\n",
    "        label = 'phishing' if int(pred[0])==1 else 'legit'\n",
    "        score = float(pred[0])\n",
    "    return {'url': url, 'prediction': label, 'confidence': round(score,4)}\n",
    "PY\n",
    "\n",
    "print('Wrote model_server.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484a38e1",
   "metadata": {},
   "source": [
    "## 8) Build a FastAPI inference endpoint\n",
    "\n",
    "We'll create a minimal FastAPI app in this notebook (written to `model_server.py`) that loads the model and exposes /health and /predict endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256da867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small tests directory and files programmatically (for demonstration)\n",
    "os.makedirs('tests', exist_ok=True)\n",
    "with open('tests/test_inference.py','w') as f:\n",
    "    f.write('''\n",
    "import numpy as np\n",
    "from model_deploy_demo import extract_features, postprocess_proba\n",
    "\n",
    "def test_extract_features():\n",
    "    f = extract_features('http://a.b')\n",
    "    assert f.shape == (1,6)\n",
    "\n",
    "def test_postprocess():\n",
    "    label, score = postprocess_proba(np.array([[0.2,0.8]]))\n",
    "    assert label == 'phishing'\n",
    "''')\n",
    "\n",
    "print('Wrote tests/tests_inference.py')\n",
    "\n",
    "# Run pytest\n",
    "print('\\nTo run tests from terminal:')\n",
    "print('pytest -q tests')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5a8fd",
   "metadata": {},
   "source": [
    "## 7) Local inference tests and unit tests\n",
    "\n",
    "Create quick pytest tests to validate preprocessing and inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_url(url: str, model):\n",
    "    X = extract_features(url)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(X)\n",
    "        label, score = postprocess_proba(proba)\n",
    "    else:\n",
    "        pred = model.predict(X)\n",
    "        # assume binary 0/1\n",
    "        label = 'phishing' if int(pred[0])==1 else 'legit'\n",
    "        score = float(pred[0])\n",
    "    return {\n",
    "        'url': url,\n",
    "        'prediction': label,\n",
    "        'confidence': score\n",
    "    }\n",
    "\n",
    "# Quick local test (will warn if model not loaded)\n",
    "if 'model' in globals():\n",
    "    print('Test prediction:', predict_url(example_url, model))\n",
    "else:\n",
    "    print('Model not loaded. Skipping quick test.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953d5ce",
   "metadata": {},
   "source": [
    "## 6) Model inference function\n",
    "\n",
    "Combine preprocessing, model predict/predict_proba, and postprocessing. Handle CPU-only environments by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285742f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputSchema(BaseModel):\n",
    "    url: str\n",
    "    prediction: str\n",
    "    confidence: float\n",
    "\n",
    "\n",
    "def postprocess_proba(proba: np.ndarray, threshold: float = 0.5):\n",
    "    score = float(proba[0,1]) if proba.shape[1] > 1 else float(proba[0,0])\n",
    "    label = 'phishing' if score > threshold else 'legit'\n",
    "    return label, round(score, 4)\n",
    "\n",
    "# Example output\n",
    "print('Example postprocess:', postprocess_proba(np.array([[0.3,0.7]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233afe2",
   "metadata": {},
   "source": [
    "## 5) Define postprocessing and output schema\n",
    "\n",
    "Map model outputs to human-readable labels and format the confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8e5b13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInputSchema\u001b[39;00m(BaseModel):\n\u001b[0;32m      2\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class InputSchema(BaseModel):\n",
    "    url: str\n",
    "\n",
    "\n",
    "def extract_features(url: str):\n",
    "    return np.array([\n",
    "        len(url),\n",
    "        url.count('.'),\n",
    "        url.count('-'),\n",
    "        url.count('@'),\n",
    "        int('https' in url),\n",
    "        int('login' in url)\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "# Example\n",
    "example_url = 'https://secure-login.example.com/account'\n",
    "print('Example features:', extract_features(example_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab42fdd",
   "metadata": {},
   "source": [
    "## 4) Define input schema and preprocessing\n",
    "\n",
    "We replicate the feature extraction used during training. The Lambda and training notebooks use similar logic: URL length, dot count, dash count, @ count, https presence, suspicious tokens (e.g., 'login'). Adjust as needed to match your training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902617ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to the model artifact produced by training\n",
    "MODEL_PATH = os.path.join('..','output','model.joblib')\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print('Model artifact not found at', MODEL_PATH)\n",
    "else:\n",
    "    model = joblib.load(MODEL_PATH)\n",
    "    print('Loaded model from', MODEL_PATH)\n",
    "    try:\n",
    "        # scikit-learn style\n",
    "        print('Model type:', type(model))\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            print('Supports predict_proba')\n",
    "    except Exception as e:\n",
    "        print('Model inspection failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e8e9a",
   "metadata": {},
   "source": [
    "## 3) Load trained model artifact\n",
    "\n",
    "This section loads a pre-trained model artifact saved with `joblib`. Update `MODEL_PATH` to point to the artifact location (local or S3-extracted local path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac92c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# FastAPI imports for later\n",
    "from pydantic import BaseModel\n",
    "\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30212b05",
   "metadata": {},
   "source": [
    "## 2) Import required libraries\n",
    "\n",
    "The notebook uses the following imports for model loading, preprocessing, and serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shell check - list installed packages (run after activating .venv)\n",
    "import sys\n",
    "!python -V\n",
    "!pip freeze | sed -n '1,100p'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2923e",
   "metadata": {},
   "source": [
    "## 1) Notebook setup and environment\n",
    "\n",
    "- Python version: 3.8+ recommended (match deployment target)\n",
    "- Use a virtual environment and `requirements.txt` for reproducibility.\n",
    "\n",
    "Create a local virtual environment and install dependencies:\n",
    "\n",
    "```powershell\n",
    "python -m venv .venv\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Example `requirements.txt` (used by cells later):\n",
    "\n",
    "```\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic\n",
    "numpy\n",
    "pandas\n",
    "scikit-learn\n",
    "joblib\n",
    "pytest\n",
    "requests\n",
    "```\n",
    "\n",
    "Verify installed packages in a notebook shell cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55965f5",
   "metadata": {},
   "source": [
    "# Deploy Model: package, local API, Docker, and CI\n",
    "\n",
    "This notebook demonstrates preparing, testing, and packaging a trained ML model for deployment.\n",
    "\n",
    "Outline:\n",
    "1. Environment setup\n",
    "2. Imports\n",
    "3. Load model artifact\n",
    "4. Preprocessing & schema\n",
    "5. Postprocessing & schema\n",
    "6. Inference function\n",
    "7. Tests\n",
    "8. FastAPI app\n",
    "9. Run locally with uvicorn\n",
    "10. Dockerfile\n",
    "11. Docker build/run\n",
    "12. CI workflow\n",
    "13. Save/version artifact\n",
    "\n",
    "This notebook is intended to be runnable locally for development and to provide artifacts for deployment on AWS SageMaker or a container platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce447e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
